{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvTp4UVz9R5A"
   },
   "source": [
    "# Inference Tutorial\n",
    "\n",
    "In this tutorial you'll learn:\n",
    "- [How to load an OmniMAE model](#Load-Model)\n",
    "- [Inference with Images](#Inference-with-Images)\n",
    "- [Inference with Videos](#Inference-with-Videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S5NNdsM9R5C"
   },
   "source": [
    "### Install modules \n",
    "\n",
    "We assume that `torch` and `torchvision` have already installed using the instructions in the [README.md](https://github.com/facebookresearch/omnivore/blob/main/README.md#setup-and-installation). \n",
    "\n",
    "Please install the other dependencies required for using Omnivore models - `einops`, `pytorchvideo` and `timm`.\n",
    "\n",
    "For this tutorial, please additionally install `ipywidgets` and `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make data dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF6cS6LM9R5E"
   },
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rb8K79k79R5F"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "try:\n",
    "    from omnivore.transforms import SpatialCrop, TemporalCrop, DepthNorm\n",
    "except:\n",
    "    # need to also make the omnivore transform module available\n",
    "    !git clone https://github.com/facebookresearch/omnivore.git\n",
    "    sys.path.append(\"./omnivore\")\n",
    "\n",
    "    from omnivore.transforms import SpatialCrop, TemporalCrop, DepthNorm\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from torchvision.transforms._transforms_video import NormalizeVideo\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from ipywidgets import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def show_image(image, rescale=True, mean=IMAGENET_MEAN, std=IMAGENET_STD, title=\"\"):\n",
    "    \n",
    "    # Image must be [H, W, 3]\n",
    "    if not image.shape[2] == 3:\n",
    "        image = torch.einsum(\"chw->hwc\", image)\n",
    "        \n",
    "    # Sometimes, unnormalization has already been made\n",
    "    if rescale:\n",
    "        plt.imshow(torch.clip((image * std + mean) * 255, 0, 255).int())\n",
    "    else:\n",
    "        plt.imshow(torch.clip(image * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis(\"off\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_imvid(x, time_dim=2):\n",
    "    if x.shape[time_dim] == 1:\n",
    "        new_shape = [1] * len(x.shape)\n",
    "        new_shape[time_dim] = 2\n",
    "        x = x.repeat(new_shape)\n",
    "\n",
    "    # Duplicate an image if odd number of frames\n",
    "    if x.shape[time_dim] % 2 == 1:\n",
    "        nb_frames = x.shape[time_dim]\n",
    "        repeat_tensor = torch.ones(nb_frames, dtype=torch.int64).to(x.device)\n",
    "        repeat_tensor[0] = 2  # duplicate first image\n",
    "        # repeat_tensor[-1] = 2  # duplicate last image\n",
    "        x = x.repeat_interleave(repeat_tensor, dim=time_dim)\n",
    "        \n",
    "    return x\n",
    "\n",
    "def patchify(imgs, patch_shape):\n",
    "    \"\"\"Adapted from omnivision.losses.mae_loss.MAELoss.patchify().\"\"\"\n",
    "    assert imgs.shape[-2] == imgs.shape[-1]  # Spatial dimensions match up\n",
    "\n",
    "    # Add a dummy time dimension to 2D patches for consistency.\n",
    "    # Since it is 1, it will not affect the final number of patches\n",
    "    if len(patch_shape) == 2:\n",
    "        patch_shape = [1,] + patch_shape\n",
    "        imgs = imgs.unsqueeze(-3)\n",
    "\n",
    "    assert imgs.ndim - 2 == len(patch_shape)  # except batch and channel dims\n",
    "    for i in range(1, len(patch_shape) + 1):\n",
    "        assert (\n",
    "            imgs.shape[-i] % patch_shape[-i] == 0\n",
    "        ), f\"image shape {imgs.shape} & patch shape {patch_shape} mismatch at index {i}\"\n",
    "\n",
    "    p = patch_shape[-3]\n",
    "    q = patch_shape[-2]\n",
    "    r = patch_shape[-1]\n",
    "    t = imgs.shape[-3] // p  # temporality    \n",
    "    h = imgs.shape[-2] // q  # height\n",
    "    w = imgs.shape[-1] // r  # width\n",
    "    x = imgs.reshape(shape=(imgs.shape[0], 3, t, p, h, q, w, r))\n",
    "    x = torch.einsum(\"nctphqwr->nthwpqrc\", x)\n",
    "    x = x.reshape(shape=(imgs.shape[0], t * h * w, p * q * r, 3))\n",
    "\n",
    "    return x\n",
    "\n",
    "def unpatchify(imgs, patch_shape):\n",
    "    \"\"\"Our own function to reverse patchify.\n",
    "    \n",
    "    Adapted from https://github.com/facebookresearch/mae/blob/main/models_mae.py#L109.\n",
    "    \"\"\"\n",
    "    p = patch_shape[-3]  # temporality\n",
    "    q = patch_shape[-2]  # height\n",
    "    r = patch_shape[-1]  # width\n",
    "    \n",
    "    # h = w = int(imgs.shape[1]**.5)  # height and width\n",
    "    img_size = 224  # cheating here\n",
    "    h = img_size // q\n",
    "    w = img_size // r\n",
    "    t = imgs.shape[1] // (h*w)\n",
    "\n",
    "    x = imgs.reshape((imgs.shape[0], t, h, w, p, q, r, 3))\n",
    "    x = torch.einsum(\"nthwpqrc->nctphqwr\", x)\n",
    "    x = x.reshape((imgs.shape[0], 3, t, p, h, q, w, r))\n",
    "    x = x.reshape((imgs.shape[0], 3, t * p, h * q, w * r))\n",
    "\n",
    "    return x\n",
    "\n",
    "def convert_output(\n",
    "    pred, mask, img, patch_shape, norm_pix_loss=True, norm_pix_per_channel=True, tfm_mean=IMAGENET_MEAN, \n",
    "    tfm_std=IMAGENET_STD\n",
    "):\n",
    "    \"\"\"Our own function to convert the output of pretrained model to images.\n",
    "    \n",
    "    Adapted from omnivision.losses.mae_loss.MAELoss.compute_mae_loss().\n",
    "    \"\"\"\n",
    "    # Duplicate image if needed (pred is already replicated)\n",
    "    img = pad_imvid(img)\n",
    "    \n",
    "    # Reverse the global normalization of the input image\n",
    "    img_mean = (\n",
    "        torch.as_tensor(tfm_mean)\n",
    "        .to(img.device)\n",
    "        .reshape([1, -1] + [1] * (img.ndim - 2))\n",
    "    )\n",
    "    img_std = (\n",
    "        torch.as_tensor(tfm_std)\n",
    "        .to(img.device)\n",
    "        .reshape([1, -1] + [1] * (img.ndim - 2))\n",
    "    )\n",
    "    img = img * img_std + img_mean         \n",
    "    \n",
    "    # The output of the model for a single image is a double image\n",
    "    # so we replicate the true image\n",
    "    img_shape = img.shape\n",
    "    if len(img_shape) == 4:  # missing time dimension\n",
    "        img = einops.repeat(img, \"b c h w -> b c t h w\", t=2).to(img.device)\n",
    "    elif len(img_shape) == 5 and img_shape[2] == 1:  # single image to replicate\n",
    "        img = img[:, :, 0, :, ...]\n",
    "        img = einops.repeat(img, \"b c h w -> b c t h w\", t=2).to(img.device)\n",
    "\n",
    "    # Squeeze back RGB channels from linear output\n",
    "    pred = pred.reshape((*pred.shape[:-1], pred.shape[-1] // 3, 3))\n",
    "\n",
    "    # Unnormalize predicted patches\n",
    "    target = patchify(img, patch_shape)            \n",
    "\n",
    "    patches_dim = -2\n",
    "    if norm_pix_loss:\n",
    "        if not norm_pix_per_channel:\n",
    "            # Merge the channel with patches and compute mean\n",
    "            # over all channels of all patches.\n",
    "            # Else, will compute a mean for each channel separately\n",
    "            target = torch.flatten(target, patches_dim)\n",
    "            patches_dim = -1\n",
    "        mean = target.mean(dim=patches_dim, keepdim=True)\n",
    "        var = target.var(dim=patches_dim, keepdim=True)\n",
    "        pred = (var**0.5) * pred + mean      \n",
    "        \n",
    "    # Unmasked patches have to be replaced by those from original image\n",
    "    mask_flatten = mask.reshape(mask.shape[0], -1)     \n",
    "    pred[mask_flatten] = torch.clone(target[mask_flatten])\n",
    "\n",
    "    # Unpatchify the predicted images\n",
    "    pred = unpatchify(pred, patch_shape)       \n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQuM7a0m9R5K"
   },
   "source": [
    "# Inference with Images\n",
    "\n",
    "First we'll load an image and use the OmniMAE model to classify it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "We provide several pretrained OmniMAE models via manual download. Available models are described in [model zoo documentation](https://github.com/facebookresearch/omnivore/tree/main/omnimae).\n",
    "\n",
    "Here we are selecting the base ViT model which was trained on Something Somethingv2 and Image-Net 1K and then finetuned on Image-Net 1K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device on which to run the model\n",
    "# Set to cuda to load on GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "# Pick a pretrained model \n",
    "from omnimae.omni_mae_model import vit_base_mae_finetune_in1k\n",
    "model = vit_base_mae_finetune_in1k()\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Download the id to label mapping for the Imagenet1K dataset. This will be used to get the category label names from the predicted class ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "482YS3Rj9R5L"
   },
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json -O data/imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yxx-F569R5M"
   },
   "outputs": [],
   "source": [
    "with open(\"data/imagenet_class_index.json\", \"r\") as f:\n",
    "    imagenet_classnames = json.load(f)\n",
    "\n",
    "# Create an id to label name mapping\n",
    "imagenet_id_to_classname = {}\n",
    "for k, v in imagenet_classnames.items():\n",
    "    imagenet_id_to_classname[k] = v[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRDpifXh9R5N"
   },
   "source": [
    "### Load and visualize the image\n",
    "\n",
    "You can download the test image in the cell below or specify a path to your own image. Before passing the image into the model we need to apply some input transforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGVAZ2pE9R5O"
   },
   "outputs": [],
   "source": [
    "# Download the example image file\n",
    "!wget -O data/library.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/13-11-02-olb-by-RalfR-03.jpg/800px-13-11-02-olb-by-RalfR-03.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Lghqzew9R5P",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path = \"data/library.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxbQQu2J9R5P"
   },
   "outputs": [],
   "source": [
    "image_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize(224),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "image = image_transform(image)\n",
    "\n",
    "# For images, the model expects inputs of shape: B x 3 x T x H x W\n",
    "print(image.shape)\n",
    "image = image[None, :, None, :, ...]\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pnAKqiy9R5Q"
   },
   "source": [
    "### Run the model \n",
    "\n",
    "The transformed image can be passed through the model to get class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xf1usPrp9R5R",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    prediction = model(image.to(device))\n",
    "    pred_classes = prediction.topk(k=5).indices\n",
    "\n",
    "pred_class_names = [imagenet_id_to_classname[str(i.item())] for i in pred_classes[0]]\n",
    "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1ub-hwp9R5R"
   },
   "source": [
    "# Inference with Videos\n",
    "\n",
    "Now we'll see how to use the OmniMAE model to classify a video. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "We provide several pretrained OmniMAE models via manual download. Available models are described in [model zoo documentation](https://github.com/facebookresearch/omnivore/tree/main/omnimae).\n",
    "\n",
    "Here we are selecting the base ViT model which was trained on Something Somethingv2 and Image-Net 1K and then finetuned on SSv2 for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device on which to run the model\n",
    "# Set to cuda to load on GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "# Pick a pretrained model \n",
    "from omnimae.omni_mae_model import vit_base_mae_finetune_ssv2\n",
    "model = vit_base_mae_finetune_ssv2()\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup \n",
    "\n",
    "Download the id to label mapping for the [Something Something v2 dataset](https://developer.qualcomm.com/software/ai-datasets/something-something) and put them under `data/ssv2_labels`. \n",
    "\n",
    "This will be used to get the category label names from the predicted class ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQ14kfY89R5S"
   },
   "outputs": [],
   "source": [
    "with open(\"data/ssv2_labels/labels.json\", \"r\") as f:\n",
    "    ssv2_classnames = json.load(f)\n",
    "\n",
    "# Create an id to label name mapping\n",
    "ssv2_id_to_classname = {}\n",
    "for k, v in ssv2_classnames.items():\n",
    "    ssv2_id_to_classname[int(v)] = str(k).replace('\"', \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alMw8PVv9R5T"
   },
   "source": [
    "### Define the transformations for the input required by the model\n",
    "\n",
    "Before passing the video into the model we need to apply some input transforms and sample a clip of the correct duration.\n",
    "\n",
    "**Remark**: These are the transformations from Omnivore model. They could be (and actually, are) different for OmniMAE. However, for the purpose of this demo, this is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xg0vPreI9R5T"
   },
   "outputs": [],
   "source": [
    "num_frames = 160\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "\n",
    "video_transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=T.Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames), \n",
    "            T.Lambda(lambda x: x / 255.0),  \n",
    "            ShortSideScale(size=224),\n",
    "            NormalizeVideo(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            TemporalCrop(frames_per_clip=32, stride=40),\n",
    "            SpatialCrop(crop_size=224, num_crops=3),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45CBNmWX9R5U"
   },
   "source": [
    "### Load and visualize an example video\n",
    "\n",
    "We can test the classification of an example video from the kinetics validation set such as this [archery video](https://www.youtube.com/watch?v=3and4vWkW4s).\n",
    "\n",
    "Otherwise, you can download videos from SSv2 dataset and select one (as 74225.webm here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcXsmi0e9R5U"
   },
   "outputs": [],
   "source": [
    "# Download the example video file\n",
    "# !wget https://dl.fbaipublicfiles.com/omnivore/example_data/dance.mp4 -O data/dance.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Pg9vyCe9R5V"
   },
   "outputs": [],
   "source": [
    "# Load the example video\n",
    "# video_path = \"data/dance.mp4\" \n",
    "video_path = \"data/74225.webm\"\n",
    "\n",
    "Video.from_file(video_path, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGYZedo99R5V"
   },
   "outputs": [],
   "source": [
    "# We crop the video to a smaller resolution and duration to save RAM\n",
    "# !ffmpeg -y -ss 12 -i data/dance.mp4 -filter:v scale=224:-1 -t 1 -v 0 data/dance_cropped.mp4\n",
    "!ffmpeg -y -ss 0 -i data/74225.webm -filter:v scale=224:-1 -t 2 -v 0 data/74225_cropped.webm\n",
    "\n",
    "# video_path = \"data/dance_cropped.mp4\" \n",
    "video_path = \"data/74225_cropped.webm\"\n",
    "\n",
    "Video.from_file(video_path, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qekSOQWt9R5W"
   },
   "outputs": [],
   "source": [
    "# Initialize an EncodedVideo helper class\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# Load the desired clip and specify the start and end duration.\n",
    "# The start_sec should correspond to where the action occurs in the video\n",
    "video_data = video.get_clip(start_sec=0, end_sec=2.0)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = video_transform(video_data)\n",
    "\n",
    "# Move the inputs to the desired device\n",
    "video_inputs = video_data[\"video\"]\n",
    "\n",
    "# Take the first clip \n",
    "# The model expects inputs of shape: B x C x T x H x W\n",
    "print(video_inputs[0].shape)\n",
    "video_input = video_inputs[0][None, ...]\n",
    "print(video_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jimUjTH49R5W"
   },
   "source": [
    "### Get model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_Mq1iKd9R5W",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pass the input clip through the model \n",
    "with torch.no_grad():\n",
    "    \n",
    "    prediction = model(video_input.to(device))\n",
    "\n",
    "    # Get the predicted classes \n",
    "    pred_classes = prediction.topk(k=5).indices\n",
    "\n",
    "# Map the predicted classes to the label names\n",
    "pred_class_names = [ssv2_id_to_classname[int(i)] for i in pred_classes[0]]\n",
    "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction of Images\n",
    "\n",
    "First we'll load an image and use the OmniMAE model to reconstruct images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "We provide several pretrained OmniMAE models via manual download. Available models are described in [model zoo documentation](https://github.com/facebookresearch/omnivore/tree/main/omnimae).\n",
    "\n",
    "Here we are selecting the base ViT model which was trained on Something Somethingv2 and Image-Net 1K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device on which to run the model\n",
    "# Set to cuda to load on GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "# Pick a pretrained model \n",
    "from omnimae.omni_mae_model import vit_base_mae_pretraining\n",
    "model = vit_base_mae_pretraining()\n",
    "\n",
    "# Set to eval mode and move to desired device>\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize the image\n",
    "\n",
    "You can download the test image in the cell below or specify a path to your own image. Before passing the image into the model we need to apply some input transforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the example image file\n",
    "!wget -O data/library.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/13-11-02-olb-by-RalfR-03.jpg/800px-13-11-02-olb-by-RalfR-03.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_path = \"data/library.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize(224),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "image = image_transform(image)\n",
    "\n",
    "# For images, the model expects inputs of shape: B x 3 x T x H x W\n",
    "print(image.shape)\n",
    "# image = image[None, :, None, :, ...]\n",
    "image = einops.repeat(image, \"c h w -> b c t h w\", b=1, t=1)\n",
    "print(image.shape)\n",
    "\n",
    "# Move to GPU\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask on the image (play with the proportion to see the effect!)\n",
    "proportion_of_mask = 0.9\n",
    "\n",
    "# Mask is of hape [N, patch_layout] where patches are of shape 2x16x16\n",
    "# so here mask is of shape 2//2 x 224//16 x 224//16\n",
    "mask = torch.empty(1, 1, 14, 14, dtype=torch.bool).bernoulli_(1-proportion_of_mask)\n",
    "\n",
    "# Move to GPU\n",
    "mask = mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model \n",
    "\n",
    "The transformed image can be passed through the model to get a reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    cls_token, decoder_patch_features = model.trunk(image, mask=mask)\n",
    "    outcome = model.head(decoder_patch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_shape = [2, 16, 16]\n",
    "\n",
    "pred_imgs = convert_output(outcome, mask, image, patch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(pred_imgs.shape)\n",
    "pred_single = pred_imgs[0, :, 0].detach().cpu()\n",
    "\n",
    "show_image(pred_single, rescale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_single = image[0, :, 0].detach().cpu()\n",
    "\n",
    "show_image(image_single, rescale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction of Videos\n",
    "\n",
    "Now, we'll load a video and use the OmniMAE model to reconstruct videos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "We provide several pretrained OmniMAE models via manual download. Available models are described in [model zoo documentation](https://github.com/facebookresearch/omnivore/tree/main/omnimae).\n",
    "\n",
    "Here we are selecting the base ViT model which was trained on Something Somethingv2 and Image-Net 1K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device on which to run the model\n",
    "# Set to cuda to load on GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "# Pick a pretrained model \n",
    "from omnimae.omni_mae_model import vit_base_mae_pretraining\n",
    "model = vit_base_mae_pretraining()\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the transformations for the input required by the model\n",
    "\n",
    "Before passing the video into the model we need to apply some input transforms and sample a clip of the correct duration.\n",
    "\n",
    "**Remark**: These are the transformations from Omnivore model. They could be (and actually, are) different for OmniMAE. However, for the purpose of this demo, this is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 160\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "\n",
    "# clip_duration = (num_frames * sampling_rate) / frames_per_second\n",
    "\n",
    "video_transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=T.Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames), \n",
    "            T.Lambda(lambda x: x / 255.0),  \n",
    "            ShortSideScale(size=224),\n",
    "            NormalizeVideo(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            TemporalCrop(frames_per_clip=32, stride=40),\n",
    "            SpatialCrop(crop_size=224, num_crops=3),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize an example video\n",
    "We can test the classification of an example video from the kinetics validation set such as this [archery video](https://www.youtube.com/watch?v=3and4vWkW4s).\n",
    "\n",
    "**Remark**: this is not a video from SSv2, but it allows us to simply test the inference with OmniMAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download the example video file\n",
    "!wget https://dl.fbaipublicfiles.com/omnivore/example_data/dance.mp4 -O data/dance.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example video\n",
    "video_path = \"data/dance.mp4\" \n",
    "# video_path = \"data/74225.webm\"\n",
    "\n",
    "Video.from_file(video_path, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We crop the video to a smaller resolution and duration to save RAM\n",
    "!ffmpeg -y -ss 12 -i data/dance.mp4 -filter:v scale=224:-1 -t 1 -v 0 data/dance_cropped.mp4\n",
    "# !ffmpeg -y -ss 0 -i data/74225.webm -filter:v scale=224:-1 -t 2 -v 0 data/74225_cropped.webm\n",
    "\n",
    "video_path = \"data/dance_cropped.mp4\" \n",
    "# video_path = \"data/74225_cropped.webm\"\n",
    "\n",
    "Video.from_file(video_path, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an EncodedVideo helper class\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# Load the desired clip and specify the start and end duration.\n",
    "# The start_sec should correspond to where the action occurs in the video\n",
    "video_data = video.get_clip(start_sec=0, end_sec=2.0)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = video_transform(video_data)\n",
    "\n",
    "# Move the inputs to the desired device\n",
    "video_inputs = video_data[\"video\"]\n",
    "\n",
    "# Take the first clip \n",
    "# The model expects inputs of shape: B x C x T x H x W\n",
    "print(video_inputs[0].shape)\n",
    "video_input = video_inputs[0][None, ...]\n",
    "print(video_input.shape)\n",
    "\n",
    "# Move to GPU\n",
    "video_input = video_input.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask on the image (play with the proportion to see the effect!)\n",
    "proportion_of_mask = 0.9\n",
    "\n",
    "# Mask is of hape [N, patch_layout] where patches are of shape 2x16x16\n",
    "# so here mask is of shape 32//2 x 224//16 x 224//16\n",
    "mask = torch.empty(1, 16, 14, 14, dtype=torch.bool).bernoulli_(1-proportion_of_mask)\n",
    "\n",
    "# Move to GPU\n",
    "mask = mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model \n",
    "\n",
    "The transformed image can be passed through the model to get a reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    cls_token, decoder_patch_features = model.trunk(video_input, mask=mask)\n",
    "    outcome = model.head(decoder_patch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_shape = [2, 16, 16]\n",
    "\n",
    "pred_imgs = convert_output(outcome, mask, video_input.to(device), patch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(pred_imgs.shape)\n",
    "pred_single = pred_imgs[0, :, -1].detach().cpu()\n",
    "\n",
    "show_image(pred_single, rescale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_single = video_input[0, :, -1].detach().cpu()\n",
    "\n",
    "show_image(video_single, rescale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "colab": {
   "name": "inference_tutorial.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "rsdproject",
   "language": "python",
   "name": "rsdproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
