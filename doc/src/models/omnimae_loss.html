<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.models.omnimae_loss API documentation</title>
<meta name="description" content="Compute the loss, patchify or unpatchify results from OmniMAE." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.models.omnimae_loss</code></h1>
</header>
<section id="section-intro">
<p>Compute the loss, patchify or unpatchify results from OmniMAE.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Compute the loss, patchify or unpatchify results from OmniMAE.&#34;&#34;&#34;


import numpy as np

import torch

import einops


IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])
IMAGENET_STD = np.array([0.229, 0.224, 0.225])


def patchify(imgs, patch_shape=[2, 16, 16], nb_channels=3):
    &#34;&#34;&#34;Adapted from omnivision.losses.mae_loss.MAELoss.patchify().&#34;&#34;&#34;
    assert imgs.shape[-2] == imgs.shape[-1]  # Spatial dimensions match up

    # Add a dummy time dimension to 2D patches for consistency
    # Since it is 1, it will not affect the final number of patches
    if len(patch_shape) == 2:
        patch_shape = [1,] + patch_shape
        imgs = imgs.unsqueeze(-3)

    assert imgs.ndim - 2 == len(patch_shape)  # except batch and channel dims
    for i in range(1, len(patch_shape) + 1):
        assert (
            imgs.shape[-i] % patch_shape[-i] == 0
        ), f&#34;image shape {imgs.shape} &amp; patch shape {patch_shape} mismatch at index {i}&#34;

    p = patch_shape[-3]
    q = patch_shape[-2]
    r = patch_shape[-1]
    t = imgs.shape[-3] // p  # temporality
    h = imgs.shape[-2] // q  # height
    w = imgs.shape[-1] // r  # width
    c = nb_channels

    x = imgs.reshape(shape=(imgs.shape[0], c, t, p, h, q, w, r))
    x = torch.einsum(&#34;nctphqwr-&gt;nthwpqrc&#34;, x)
    patchified_imgs = x.reshape(shape=(imgs.shape[0], t * h * w, p * q * r, c))

    return patchified_imgs


def unpatchify(
    patchified_imgs, patch_shape=[2, 16, 16], nb_channels=3, img_height=224, img_width=224
):
    &#34;&#34;&#34;Our own function to reverse patchify.

    Adapted from https://github.com/facebookresearch/mae/blob/main/models_mae.py#L109.
    &#34;&#34;&#34;
    p = patch_shape[-3]  # temporality
    q = patch_shape[-2]  # height
    r = patch_shape[-1]  # width
    h = img_height // q
    w = img_width // r
    t = patchified_imgs.shape[1] // (h*w)
    c = nb_channels

    x = patchified_imgs.reshape((patchified_imgs.shape[0], t, h, w, p, q, r, c))
    x = torch.einsum(&#34;nthwpqrc-&gt;nctphqwr&#34;, x)
    x = x.reshape((patchified_imgs.shape[0], c, t, p, h, q, w, r))
    imgs = x.reshape((patchified_imgs.shape[0], c, t * p, h * q, w * r))

    return imgs


def compute_images_and_loss(
    pred_imgs_np, true_imgs_ng, mask, replace_mask=False, patch_shape=[2, 16, 16], nb_channels=3,
    img_height=224, img_width=224, norm_pix_loss=True, norm_pix_per_channel=True,
    trsf_mean=IMAGENET_MEAN, trsf_std=IMAGENET_STD
):
    &#34;&#34;&#34;Our own function to convert the output of pretrained model to images.

    Adapted from omnivision.losses.mae_loss.MAELoss.compute_mae_loss().

    The suffix &#34;n&#34; means normalized and the suffix &#34;un&#34; means unnormalized.
    The suffix &#34;g&#34; means globally and the suffix &#34;p&#34; means per patch.
    OmniMAE outputs images which are normalized per patch wrt the input images.
    &#34;&#34;&#34;
    # Reverse the global normalization of the true images
    img_mean = (
        torch.as_tensor(trsf_mean)
        .to(true_imgs_ng.device)
        .reshape([1, -1] + [1] * (true_imgs_ng.ndim - 2))
    )
    img_std = (
        torch.as_tensor(trsf_std)
        .to(true_imgs_ng.device)
        .reshape([1, -1] + [1] * (true_imgs_ng.ndim - 2))
    )
    true_imgs_ung = true_imgs_ng * img_std + img_mean

    # Replicate the true images - deprecated
    img_shape = true_imgs_ung.shape
    if len(img_shape) == 4:  # missing time dimension
        true_imgs_ung = einops.repeat(true_imgs_ung, &#34;b c h w -&gt; b c t h w&#34;, t=2)
        true_imgs_ung = true_imgs_ung.to(true_imgs_ng.device)
    elif len(img_shape) == 5 and img_shape[2] == 1:  # single image to replicate
        true_imgs_ung = true_imgs_ung[:, :, 0, :, ...]
        true_imgs_ung = einops.repeat(true_imgs_ung, &#34;b c h w -&gt; b c t h w&#34;, t=2)
        true_imgs_ung = true_imgs_ung.to(true_imgs_ng.device)

    # Patchify true images
    target = patchify(true_imgs_ung, patch_shape=patch_shape, nb_channels=nb_channels)

    # Squeeze back channels from linear output
    channels_dim = 1
    nb_channels = true_imgs_ung.shape[channels_dim]

    pred_imgs_np = pred_imgs_np.reshape(
        (*pred_imgs_np.shape[:-1], pred_imgs_np.shape[-1] // nb_channels, nb_channels)
    )

    # Unnormalize predicted patches and compute the loss
    patches_dim = -2

    if norm_pix_loss:

        if not norm_pix_per_channel:

            # Merge the channel with patches and compute mean over all channels of all patches
            # Else, will compute a mean for each channel separately
            target = torch.flatten(target, patches_dim)
            patches_dim = -1

        mean = target.mean(dim=patches_dim, keepdim=True)
        var = target.var(dim=patches_dim, keepdim=True)

        pred_imgs_unp = (var**0.5) * pred_imgs_np + mean

        target_np = (target - mean) / (var + 1.0e-6) ** 0.5

    loss_imgs = (pred_imgs_np - target_np) ** 2

    # Replace non-masked patches to those from original images for better visualization
    if replace_mask:

        mask_flatten = mask.reshape(mask.shape[0], -1)

        pred_imgs_unp[mask_flatten] = torch.clone(target[mask_flatten])
        loss_imgs[mask_flatten] = 0.

    # Unpatchify the predicted images
    pred_imgs_unp = unpatchify(
        pred_imgs_unp, patch_shape=patch_shape, nb_channels=nb_channels, img_height=img_height,
        img_width=img_width
    )
    loss_imgs = unpatchify(
        loss_imgs, patch_shape=patch_shape, nb_channels=nb_channels, img_height=img_height,
        img_width=img_width
    )

    return pred_imgs_unp, true_imgs_ung, loss_imgs</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.models.omnimae_loss.compute_images_and_loss"><code class="name flex">
<span>def <span class="ident">compute_images_and_loss</span></span>(<span>pred_imgs_np, true_imgs_ng, mask, replace_mask=False, patch_shape=[2, 16, 16], nb_channels=3, img_height=224, img_width=224, norm_pix_loss=True, norm_pix_per_channel=True, trsf_mean=array([0.485, 0.456, 0.406]), trsf_std=array([0.229, 0.224, 0.225]))</span>
</code></dt>
<dd>
<div class="desc"><p>Our own function to convert the output of pretrained model to images.</p>
<p>Adapted from omnivision.losses.mae_loss.MAELoss.compute_mae_loss().</p>
<p>The suffix "n" means normalized and the suffix "un" means unnormalized.
The suffix "g" means globally and the suffix "p" means per patch.
OmniMAE outputs images which are normalized per patch wrt the input images.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_images_and_loss(
    pred_imgs_np, true_imgs_ng, mask, replace_mask=False, patch_shape=[2, 16, 16], nb_channels=3,
    img_height=224, img_width=224, norm_pix_loss=True, norm_pix_per_channel=True,
    trsf_mean=IMAGENET_MEAN, trsf_std=IMAGENET_STD
):
    &#34;&#34;&#34;Our own function to convert the output of pretrained model to images.

    Adapted from omnivision.losses.mae_loss.MAELoss.compute_mae_loss().

    The suffix &#34;n&#34; means normalized and the suffix &#34;un&#34; means unnormalized.
    The suffix &#34;g&#34; means globally and the suffix &#34;p&#34; means per patch.
    OmniMAE outputs images which are normalized per patch wrt the input images.
    &#34;&#34;&#34;
    # Reverse the global normalization of the true images
    img_mean = (
        torch.as_tensor(trsf_mean)
        .to(true_imgs_ng.device)
        .reshape([1, -1] + [1] * (true_imgs_ng.ndim - 2))
    )
    img_std = (
        torch.as_tensor(trsf_std)
        .to(true_imgs_ng.device)
        .reshape([1, -1] + [1] * (true_imgs_ng.ndim - 2))
    )
    true_imgs_ung = true_imgs_ng * img_std + img_mean

    # Replicate the true images - deprecated
    img_shape = true_imgs_ung.shape
    if len(img_shape) == 4:  # missing time dimension
        true_imgs_ung = einops.repeat(true_imgs_ung, &#34;b c h w -&gt; b c t h w&#34;, t=2)
        true_imgs_ung = true_imgs_ung.to(true_imgs_ng.device)
    elif len(img_shape) == 5 and img_shape[2] == 1:  # single image to replicate
        true_imgs_ung = true_imgs_ung[:, :, 0, :, ...]
        true_imgs_ung = einops.repeat(true_imgs_ung, &#34;b c h w -&gt; b c t h w&#34;, t=2)
        true_imgs_ung = true_imgs_ung.to(true_imgs_ng.device)

    # Patchify true images
    target = patchify(true_imgs_ung, patch_shape=patch_shape, nb_channels=nb_channels)

    # Squeeze back channels from linear output
    channels_dim = 1
    nb_channels = true_imgs_ung.shape[channels_dim]

    pred_imgs_np = pred_imgs_np.reshape(
        (*pred_imgs_np.shape[:-1], pred_imgs_np.shape[-1] // nb_channels, nb_channels)
    )

    # Unnormalize predicted patches and compute the loss
    patches_dim = -2

    if norm_pix_loss:

        if not norm_pix_per_channel:

            # Merge the channel with patches and compute mean over all channels of all patches
            # Else, will compute a mean for each channel separately
            target = torch.flatten(target, patches_dim)
            patches_dim = -1

        mean = target.mean(dim=patches_dim, keepdim=True)
        var = target.var(dim=patches_dim, keepdim=True)

        pred_imgs_unp = (var**0.5) * pred_imgs_np + mean

        target_np = (target - mean) / (var + 1.0e-6) ** 0.5

    loss_imgs = (pred_imgs_np - target_np) ** 2

    # Replace non-masked patches to those from original images for better visualization
    if replace_mask:

        mask_flatten = mask.reshape(mask.shape[0], -1)

        pred_imgs_unp[mask_flatten] = torch.clone(target[mask_flatten])
        loss_imgs[mask_flatten] = 0.

    # Unpatchify the predicted images
    pred_imgs_unp = unpatchify(
        pred_imgs_unp, patch_shape=patch_shape, nb_channels=nb_channels, img_height=img_height,
        img_width=img_width
    )
    loss_imgs = unpatchify(
        loss_imgs, patch_shape=patch_shape, nb_channels=nb_channels, img_height=img_height,
        img_width=img_width
    )

    return pred_imgs_unp, true_imgs_ung, loss_imgs</code></pre>
</details>
</dd>
<dt id="src.models.omnimae_loss.patchify"><code class="name flex">
<span>def <span class="ident">patchify</span></span>(<span>imgs, patch_shape=[2, 16, 16], nb_channels=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Adapted from omnivision.losses.mae_loss.MAELoss.patchify().</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def patchify(imgs, patch_shape=[2, 16, 16], nb_channels=3):
    &#34;&#34;&#34;Adapted from omnivision.losses.mae_loss.MAELoss.patchify().&#34;&#34;&#34;
    assert imgs.shape[-2] == imgs.shape[-1]  # Spatial dimensions match up

    # Add a dummy time dimension to 2D patches for consistency
    # Since it is 1, it will not affect the final number of patches
    if len(patch_shape) == 2:
        patch_shape = [1,] + patch_shape
        imgs = imgs.unsqueeze(-3)

    assert imgs.ndim - 2 == len(patch_shape)  # except batch and channel dims
    for i in range(1, len(patch_shape) + 1):
        assert (
            imgs.shape[-i] % patch_shape[-i] == 0
        ), f&#34;image shape {imgs.shape} &amp; patch shape {patch_shape} mismatch at index {i}&#34;

    p = patch_shape[-3]
    q = patch_shape[-2]
    r = patch_shape[-1]
    t = imgs.shape[-3] // p  # temporality
    h = imgs.shape[-2] // q  # height
    w = imgs.shape[-1] // r  # width
    c = nb_channels

    x = imgs.reshape(shape=(imgs.shape[0], c, t, p, h, q, w, r))
    x = torch.einsum(&#34;nctphqwr-&gt;nthwpqrc&#34;, x)
    patchified_imgs = x.reshape(shape=(imgs.shape[0], t * h * w, p * q * r, c))

    return patchified_imgs</code></pre>
</details>
</dd>
<dt id="src.models.omnimae_loss.unpatchify"><code class="name flex">
<span>def <span class="ident">unpatchify</span></span>(<span>patchified_imgs, patch_shape=[2, 16, 16], nb_channels=3, img_height=224, img_width=224)</span>
</code></dt>
<dd>
<div class="desc"><p>Our own function to reverse patchify.</p>
<p>Adapted from <a href="https://github.com/facebookresearch/mae/blob/main/models_mae.py#L109.">https://github.com/facebookresearch/mae/blob/main/models_mae.py#L109.</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unpatchify(
    patchified_imgs, patch_shape=[2, 16, 16], nb_channels=3, img_height=224, img_width=224
):
    &#34;&#34;&#34;Our own function to reverse patchify.

    Adapted from https://github.com/facebookresearch/mae/blob/main/models_mae.py#L109.
    &#34;&#34;&#34;
    p = patch_shape[-3]  # temporality
    q = patch_shape[-2]  # height
    r = patch_shape[-1]  # width
    h = img_height // q
    w = img_width // r
    t = patchified_imgs.shape[1] // (h*w)
    c = nb_channels

    x = patchified_imgs.reshape((patchified_imgs.shape[0], t, h, w, p, q, r, c))
    x = torch.einsum(&#34;nthwpqrc-&gt;nctphqwr&#34;, x)
    x = x.reshape((patchified_imgs.shape[0], c, t, p, h, q, w, r))
    imgs = x.reshape((patchified_imgs.shape[0], c, t * p, h * q, w * r))

    return imgs</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.models" href="index.html">src.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.models.omnimae_loss.compute_images_and_loss" href="#src.models.omnimae_loss.compute_images_and_loss">compute_images_and_loss</a></code></li>
<li><code><a title="src.models.omnimae_loss.patchify" href="#src.models.omnimae_loss.patchify">patchify</a></code></li>
<li><code><a title="src.models.omnimae_loss.unpatchify" href="#src.models.omnimae_loss.unpatchify">unpatchify</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>